{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a138279f-3a37-41c8-934e-d54a76625c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importar SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "# crear objeto de sesión spark\n",
    "spark=SparkSession.builder.appName(\"Feature engineering\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9ab557-3530-4486-bfd2-5c4330175055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n|emp_id|salario|\n+------+-------+\n|   100| 120000|\n|   200| 170000|\n|   300| 150000|\n+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "column_names = [\"emp_id\", \"salario\"]\n",
    "records = [(100, 120000), (200, 170000), (300, 150000)]\n",
    "df = spark.createDataFrame(records, schema=column_names)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d0b0e3-b8e6-4ab9-96e9-6140e7ea7634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n|emp_id|salario| bonos|\n+------+-------+------+\n|   100| 120000|6000.0|\n|   200| 170000|8500.0|\n|   300| 150000|7500.0|\n+------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"bonos\", df.salario * 0.05)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dba910d-5046-4fdf-b192-15591a82090d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+\n|emp_id|salario|tripled_col|\n+------+-------+-----------+\n|   100| 120000|     600000|\n|   200| 170000|     850000|\n|   300| 150000|     750000|\n+------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Cargamo l función udf\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Creamos una función \n",
    "@udf(\"integer\")\n",
    "def tripled(num):\n",
    "    return 5*int(num)\n",
    "\n",
    "# Agregamo una nueva variable DAtaFrame\n",
    "df2 = df.withColumn('tripled_col', tripled(df.salario))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b67fa639-1ea5-436d-80b3-03b97a98a3f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n|Seqno|Nombre        |\n+-----+--------------+\n|1    |juan Jones    |\n|2    |tracey aguilar|\n|3    |amy castellon |\n+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "columns = [\"Seqno\",\"Nombre\"]\n",
    "data = [(\"1\", \"juan Jones\"),\n",
    "    (\"2\", \"tracey aguilar\"),\n",
    "    (\"3\", \"amy castellon\")]\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "333ebd4a-6587-4361-a8d8-bc8caa9eae9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54d4ca70-187b-4768-af7a-201768e6493d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convertiendo la función a UDF \n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bde092c0-c81c-420d-932d-942e711689bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n|Seqno|        Nombre|\n+-----+--------------+\n|    1|    juan Jones|\n|    2|tracey aguilar|\n|    3| amy castellon|\n+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550e646d-19c0-43f0-830e-668fe451db82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n|Seqno|Nombre_Mayus   |\n+-----+---------------+\n|1    |Juan Jones     |\n|2    |Tracey Aguilar |\n|3    |Amy Castellon  |\n+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"nombre\")).alias(\"Nombre_Mayus\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4537a292-01c8-4cf3-9b05-53c7dd54392c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark: Una instancia en SparkSession\n",
    "# creamos un DataFrame\n",
    "df = spark.createDataFrame([\n",
    "(1, 'CS', 'MS'),\n",
    "(2, 'MATH', 'PHD'),\n",
    "(3, 'MATH', 'MS'),\n",
    "(4, 'CS', 'MS'),\n",
    "(5, 'CS', 'PHD'),\n",
    "(6, 'ECON', 'BS'), (7, 'ECON', 'BS'),], ['id', 'dept', 'education'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a13cb95f-a71e-45d7-82a5-9ce57342b20a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+\n| id|dept|education|\n+---+----+---------+\n|  1|  CS|       MS|\n|  2|MATH|      PHD|\n|  3|MATH|       MS|\n|  4|  CS|       MS|\n|  5|  CS|      PHD|\n|  6|ECON|       BS|\n|  7|ECON|       BS|\n+---+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431cb364-954d-427e-90f1-e986888c52e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c0b4eb1-a604-491f-9dcc-09b7afb91400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stage 1: transforma el `dept` columna a numerica\n",
    "stage_1 = StringIndexer(inputCol= 'dept', outputCol= 'dept_index')\n",
    "#\n",
    "# Stage 2: transforma la `education` a columna numérica\n",
    "stage_2 = StringIndexer(inputCol= 'education', outputCol= 'education_index')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59368331-0a72-482a-82a5-714079bbd0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+----------+\n| id|dept|education|dept_index|\n+---+----+---------+----------+\n|  1|  CS|       MS|       0.0|\n|  2|MATH|      PHD|       2.0|\n|  3|MATH|       MS|       2.0|\n|  4|  CS|       MS|       0.0|\n|  5|  CS|      PHD|       0.0|\n|  6|ECON|       BS|       1.0|\n|  7|ECON|       BS|       1.0|\n+---+----+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aplicar la transformación\n",
    "stage_1.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb244b08-dea4-400f-b7e9-6bff2af214d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+---------------+\n| id|dept|education|education_index|\n+---+----+---------+---------------+\n|  1|  CS|       MS|            0.0|\n|  2|MATH|      PHD|            2.0|\n|  3|MATH|       MS|            0.0|\n|  4|  CS|       MS|            0.0|\n|  5|  CS|      PHD|            2.0|\n|  6|ECON|       BS|            1.0|\n|  7|ECON|       BS|            1.0|\n+---+----+---------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Aplicar la transformación\n",
    "stage_2.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61223f3-ae17-459e-ae53-cea71b635e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Etapa 3: codificar one-hot la columna numérica `education_index`\n",
    "stage_3 = OneHotEncoder(inputCols=['education_index'],outputCols=['education_OHE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "466f72f5-e0ab-4926-9219-84052d7ca5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+\n| id|dept|education|\n+---+----+---------+\n|  1|  CS|       MS|\n|  2|MATH|      PHD|\n|  3|MATH|       MS|\n|  4|  CS|       MS|\n|  5|  CS|      PHD|\n|  6|ECON|       BS|\n|  7|ECON|       BS|\n+---+----+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd7b7edb-6894-4efc-8a9b-f7355997ef11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# configurar la canalización: pegar las etapas juntas\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3 ])# Entubar todas las etapas\n",
    "# ajuste el modelo de tubería y transforme los datos como se define\n",
    "pipeline_model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57659d2-20e0-41d0-aefa-ff501b3f880e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+----------+---------------+-------------+\n|id |dept|education|dept_index|education_index|education_OHE|\n+---+----+---------+----------+---------------+-------------+\n|1  |CS  |MS       |0.0       |0.0            |(2,[0],[1.0])|\n|2  |MATH|PHD      |2.0       |2.0            |(2,[],[])    |\n|3  |MATH|MS       |2.0       |0.0            |(2,[0],[1.0])|\n|4  |CS  |MS       |0.0       |0.0            |(2,[0],[1.0])|\n|5  |CS  |PHD      |0.0       |2.0            |(2,[],[])    |\n|6  |ECON|BS       |1.0       |1.0            |(2,[1],[1.0])|\n|7  |ECON|BS       |1.0       |1.0            |(2,[1],[1.0])|\n+---+----+---------+----------+---------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Se realiza la tranformación\n",
    "final_df = pipeline_model.transform(df) # Se aplica nuestro datos\n",
    "final_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f8a884c-0748-4c04-a7df-7be3ff71161b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|feature|\n+---+-------+\n|  1|    0.1|\n|  2|    0.2|\n|  3|    0.5|\n|  4|    0.8|\n|  5|    0.9|\n|  6|    1.1|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "raw_df = spark.createDataFrame([\n",
    "(1, 0.1),\n",
    "(2, 0.2),\n",
    "(3, 0.5),\n",
    "(4, 0.8),\n",
    "(5, 0.9),\n",
    "(6, 1.1)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fec4adb-b490-4623-b6ba-97a363726241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\",outputCol=\"binarized_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35c64b5-78b3-481d-95c7-849014ce58c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida del binarizador con Threshold = 0.500000\n"
     ]
    }
   ],
   "source": [
    "binarized_df = binarizer.transform(raw_df)\n",
    "print(\"Salida del binarizador con Threshold = %f\" % binarizer.getThreshold())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28faf22d-f93d-4f70-bbd1-49cb3500b5a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------------+\n|id |feature|binarized_feature|\n+---+-------+-----------------+\n|1  |0.1    |0.0              |\n|2  |0.2    |0.0              |\n|3  |0.5    |0.0              |\n|4  |0.8    |1.0              |\n|5  |0.9    |1.0              |\n|6  |1.1    |1.0              |\n+---+-------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#binarized_df = binarizer.transform(raw_df)\n",
    "binarized_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9e8b78-79e9-4704-8939-7e13eeebf3b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n|id |col1|col2|\n+---+----+----+\n|1  |12.0|5.0 |\n|2  |7.0 |10.0|\n|3  |10.0|12.0|\n|4  |5.0 |NaN |\n|5  |6.0 |null|\n|6  |NaN |NaN |\n|7  |null|null|\n+---+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 12.0, 5.0),(2, 7.0, 10.0),(3, 10.0, 12.0),(4, 5.0, float(\"nan\")),(5, 6.0, None),\n",
    "                            (6, float(\"nan\"), float(\"nan\")),(7, None, None)], [\"id\", \"col1\", \"col2\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea53188e-ffaa-4026-ad28-1b0a6ee81bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+\n|id |col1|col2|col1_out|col2_out|\n+---+----+----+--------+--------+\n|1  |12.0|5.0 |12.0    |5.0     |\n|2  |7.0 |10.0|7.0     |10.0    |\n|3  |10.0|12.0|10.0    |12.0    |\n|4  |5.0 |NaN |5.0     |9.0     |\n|5  |6.0 |null|6.0     |9.0     |\n|6  |NaN |NaN |8.0     |9.0     |\n|7  |null|null|8.0     |9.0     |\n+---+----+----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# LLamamos a la función Imputer\n",
    "from pyspark.ml.feature import Imputer\n",
    "# Por defauld usa la estrategia de la media\n",
    "imputer = Imputer(inputCols=[\"col1\", \"col2\"],outputCols=[\"col1_out\", \"col2_out\"])\n",
    "# Realiza la imputación\n",
    "model = imputer.fit(df)\n",
    "transformed = model.transform(df)\n",
    "# Muestra el resultado\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "561da041-f5d8-457b-a1f9-3dc758fc6b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+\n|id |col1|col2|col1_out|col2_out|\n+---+----+----+--------+--------+\n|1  |12.0|5.0 |12.0    |5.0     |\n|2  |7.0 |10.0|7.0     |10.0    |\n|3  |10.0|12.0|10.0    |12.0    |\n|4  |5.0 |NaN |5.0     |10.0    |\n|5  |6.0 |null|6.0     |10.0    |\n|6  |NaN |NaN |7.0     |10.0    |\n|7  |null|null|7.0     |10.0    |\n+---+----+----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Estrategia con la mediana\n",
    "imputer.setStrategy(\"median\")\n",
    "# Rellena con la mediana\n",
    "model = imputer.fit(df)\n",
    "transformed = model.transform(df)\n",
    "# Muestra el resultado\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc6c0833-08aa-4f14-9c2a-07bd7459c1a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a88f20e-1564-4557-829c-29aa64557d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n|id |texto                |\n+---+---------------------+\n|1  |a Fox jumped over FOX|\n|2  |RED of fox jumped    |\n+---+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "docs = [(1, \"a Fox jumped over FOX\"),(2, \"RED of fox jumped\")]\n",
    "df = spark.createDataFrame(docs, [\"id\", \"texto\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a430293-1d4b-4b14-ac9f-dccea5a0bb5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+---------------------------+\n|id |texto                |tokens                     |\n+---+---------------------+---------------------------+\n|1  |a Fox jumped over FOX|[a, fox, jumped, over, fox]|\n|2  |RED of fox jumped    |[red, of, fox, jumped]     |\n+---+---------------------+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Le decimos cual es la columna o variable que desea Tokenizer y la salida\n",
    "tokenizer = Tokenizer(inputCol=\"texto\", outputCol=\"tokens\")\n",
    "# Se realiza el Tokenizer\n",
    "tokenized = tokenizer.transform(df)\n",
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec401b3-a268-4174-a1c9-ee66aab585f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------------+-------------+\n|texto                |tokens                     |tokens_length|\n+---------------------+---------------------------+-------------+\n|a Fox jumped over FOX|[a, fox, jumped, over, fox]|5            |\n|RED of fox jumped    |[red, of, fox, jumped]     |4            |\n+---------------------+---------------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized.select(\"texto\", \"tokens\").withColumn(\"tokens_length\",countTokens(col(\"tokens\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6b5885-b1c1-4b09-99ed-849cd6cfc4e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------------------------+-------------+\n|texto                |tokens                  |tokens_length|\n+---------------------+------------------------+-------------+\n|a Fox jumped over FOX|[fox, jumped, over, fox]|4            |\n|RED of fox jumped    |[red, fox, jumped]      |3            |\n+---------------------+------------------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"texto\", outputCol=\"tokens\",\n",
    "pattern=\"\\\\W\", minTokenLength=3)\n",
    "# Relizame la tranformación\n",
    "regex_tokenized = regexTokenizer.transform(df)\n",
    "regex_tokenized.select(\"texto\", \"tokens\").withColumn(\"tokens_length\", countTokens(col(\"tokens\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e82c750-694e-4a54-81ad-5afdb6a8b58f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n|id |text                          |\n+---+------------------------------+\n|1  |a Fox jumped, over, the fence?|\n|2  |a RED, of fox?                |\n|3  |Curso Pyspark Bolivia         |\n+---+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "docs = [(1, \"a Fox jumped, over, the fence?\"),(2, \"a RED, of fox?\"),(3, \"Curso Pyspark Bolivia\")]\n",
    "df = spark.createDataFrame(docs, [\"id\", \"text\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e97d6f-efc7-4d1a-b02f-41dbc4575f60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+----------------------------------+-------------------------+\n|id |text                          |text2                             |text3                    |\n+---+------------------------------+----------------------------------+-------------------------+\n|1  |a Fox jumped, over, the fence?|[a, fox, jumped, over, the, fence]|[fox, jumped, fence]     |\n|2  |a RED, of fox?                |[a, red, of, fox]                 |[red, fox]               |\n|3  |Curso Pyspark Bolivia         |[curso, pyspark, bolivia]         |[curso, pyspark, bolivia]|\n+---+------------------------------+----------------------------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "tk = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=\"text\",outputCol='text2')\n",
    "sw = StopWordsRemover(inputCol='text2', outputCol='text3')\n",
    "# Creamo nuestro Pipline\n",
    "pipeline = Pipeline(stages=[tk, sw])\n",
    "df4 = pipeline.fit(df).transform(df)\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1761aee7-849f-4174-b46e-19935ce9e054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n|nombre|edad|\n+------+----+\n|  alex|   1|\n|  jans|   3|\n|   ali|   6|\n| bruno|  10|\n+------+----+\n\n"
     ]
    }
   ],
   "source": [
    "features = [('alex', 1), ('jans', 3), ('ali', 6), ('bruno', 10)]\n",
    "columns = (\"nombre\", \"edad\")\n",
    "samples = spark.createDataFrame(features, columns)\n",
    "samples.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0bd35a-dc81-4833-b359-920e1ad1a2b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------+----+-------------------+\n|mean_edad|stddev_edad       |nombre|edad|edad_scaled        |\n+---------+------------------+------+----+-------------------+\n|5.0      |3.9157800414902435|alex  |1   |-1.0215078369104984|\n|5.0      |3.9157800414902435|jans  |3   |-0.5107539184552492|\n|5.0      |3.9157800414902435|ali   |6   |0.2553769592276246 |\n|5.0      |3.9157800414902435|bruno |10  |1.276884796138123  |\n+---------+------------------+------+----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev, mean, col\n",
    "\n",
    "(samples.select(mean(\"edad\").alias(\"mean_edad\"),\n",
    "                stddev(\"edad\").alias(\"stddev_edad\")).crossJoin(samples).withColumn(\"edad_scaled\",\n",
    "                                                                                 (col(\"edad\") - col(\"mean_edad\")) / col(\"stddev_edad\"))) .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cfd7671-5de3-4589-8953-61b5575fccdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------------------+\n|nombre|edad|edad_scaled        |\n+------+----+-------------------+\n|alex  |1   |-1.0215078369104984|\n|jans  |3   |-0.5107539184552492|\n|ali   |6   |0.2553769592276246 |\n|bruno |10  |1.276884796138123  |\n+------+----+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "mean_age, sttdev_age = samples.select(mean(\"edad\"), stddev(\"edad\")).first()\n",
    "samples.withColumn(\"edad_scaled\",(col(\"edad\") - mean_age) / sttdev_age).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88682d2b-4c2d-45ac-80d4-fad4be552609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+\n|nombre|edad|edad_vector|\n+------+----+-----------+\n|  alex|   1|      [1.0]|\n|  jans|   3|      [3.0]|\n|   ali|   6|      [6.0]|\n| bruno|  10|     [10.0]|\n+------+----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "vecAssembler = VectorAssembler(inputCols=['edad'], outputCol=\"edad_vector\")\n",
    "samples2 = vecAssembler.transform(samples)\n",
    "samples2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c995f2-b23d-4cb3-8adf-051003c5246e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----------+---------------------+\n|nombre|edad|edad_vector|edad_scaled          |\n+------+----+-----------+---------------------+\n|alex  |1   |[1.0]      |[-1.0215078369104984]|\n|jans  |3   |[3.0]      |[-0.5107539184552492]|\n|ali   |6   |[6.0]      |[0.2553769592276246] |\n|bruno |10  |[10.0]     |[1.276884796138123]  |\n+------+----+-----------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler(inputCol=\"edad_vector\", outputCol=\"edad_scaled\",withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(samples2)\n",
    "scaledData = scalerModel.transform(samples2)\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e658064c-c853-4f12-913d-fe1d837b8b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de Scaling :\n+-------+-------+--------+\n|user_id|revenue|num_days|\n+-------+-------+--------+\n|    100|  77560|      45|\n|    200|  41560|      23|\n|    300|  30285|      20|\n|    400|  10345|       6|\n|    500|  88000|      50|\n+-------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([ (100, 77560, 45),(200, 41560, 23),(300, 30285, 20),\n",
    "                            (400, 10345, 6),(500, 88000, 50)], \n",
    "                           [\"user_id\", \"revenue\",\"num_days\"])\n",
    "print(\"Antes de Scaling :\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d929e7fb-63b1-4b3c-b6cc-6431e5016ed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler # Para la normalización\n",
    "from pyspark.ml.feature import VectorAssembler#  convierte las variables en vector\n",
    "from pyspark.ml import Pipeline # Entuva los procesos\n",
    "from pyspark.sql.functions import udf # Función definiada por el usuario\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# UDF para convertir el tipo de columna de vector a tipo doble\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74b9372f-e77a-4d11-bb3b-40aa51427129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+---------------+\n|user_id|revenue|num_days|num_days_Scaled|\n+-------+-------+--------+---------------+\n|    100|  77560|      45|          0.886|\n|    200|  41560|      23|          0.386|\n|    300|  30285|      20|          0.318|\n|    400|  10345|       6|            0.0|\n|    500|  88000|      50|            1.0|\n+-------+-------+--------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Iterando sobre las columnas a escalar\n",
    "for i in [\"revenue\",\"num_days\"]:\n",
    "    # Transformación VectorAssembler: conversión de columna a tipo vectorial\n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "    # MinMaxScaler transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "    # Pipeline y VectorAssembler y MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    # Fitting pipeline on DataFrame\n",
    "df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "    \n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e227f1-075b-476c-ac5a-21a03b3ab08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n|  x|  y|   z|\n+---+---+----+\n|  0|  1| 100|\n|  1|  2| 200|\n|  2|  5|1000|\n+---+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "triplets = [(0, 1, 100), (1, 2, 200), (2, 5, 1000)]\n",
    "df = spark.createDataFrame(triplets, ['x', 'y', 'z'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95862f6c-03a5-49f5-ba74-407d7e8a23ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+--------+--------+\n|x  |y  |z   |x_vector|x_scaled|\n+---+---+----+--------+--------+\n|0  |1  |100 |[0.0]   |[0.0]   |\n|1  |2  |200 |[1.0]   |[0.5]   |\n|2  |5  |1000|[2.0]   |[1.0]   |\n+---+---+----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"x\"], outputCol=\"x_vector\")\n",
    "scaler = MinMaxScaler(inputCol=\"x_vector\", outputCol=\"x_scaled\")\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "scalerModel = pipeline.fit(df)\n",
    "scaledData = scalerModel.transform(df)\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde47d69-27e3-49bb-9982-9fe051da46e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n|  x|  y|   z|\n+---+---+----+\n|  0|  1| 100|\n|  1|  2| 200|\n|  2|  5|1000|\n+---+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "triplets = [(0, 1, 100), (1, 2, 200), (2, 5, 1000)]\n",
    "df = spark.createDataFrame(triplets, ['x', 'y', 'z'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f477178d-6627-4d0e-9768-e54e880c8f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+--------+--------+--------+--------+--------+--------------------+\n|x  |y  |z   |x_vector|y_vector|z_vector|x_scaled|y_scaled|z_scaled            |\n+---+---+----+--------+--------+--------+--------+--------+--------------------+\n|0  |1  |100 |[0.0]   |[1.0]   |[100.0] |[0.0]   |[0.0]   |[0.0]               |\n|1  |2  |200 |[1.0]   |[2.0]   |[200.0] |[0.5]   |[0.25]  |[0.1111111111111111]|\n|2  |5  |1000|[2.0]   |[5.0]   |[1000.0]|[1.0]   |[1.0]   |[1.0]               |\n+---+---+----+--------+--------+--------+--------+--------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "columns_to_scale = [\"x\", \"y\", \"z\"]\n",
    "assemblers = [VectorAssembler(inputCols=[col],outputCol=col + \"_vector\") for col in columns_to_scale]\n",
    "scalers = [MinMaxScaler(inputCol=col + \"_vector\",outputCol=col + \"_scaled\") for col in columns_to_scale]\n",
    "pipeline = Pipeline(stages=assemblers + scalers)\n",
    "scalerModel = pipeline.fit(df)\n",
    "scaledData = scalerModel.transform(df)\n",
    "scaledData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15feb821-b8c2-417f-9ae9-3653dd1ec645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------------+\n|    x|     y|                   z|\n+-----+------+--------------------+\n|[0.0]| [0.0]|               [0.0]|\n|[0.5]|[0.25]|[0.1111111111111111]|\n|[1.0]| [1.0]|               [1.0]|\n+-----+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "names = {x + \"_scaled\": x for x in columns_to_scale}\n",
    "scaledData = scaledData.select([f.col(c).alias(names[c]) for c in names.keys()])\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ec6fd5-1a92-4fbe-aaee-3c1adaa5cfc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n| id|      features|\n+---+--------------+\n|  0|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]|\n|  2|[4.0,10.0,2.0]|\n+---+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n",
    "    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89ab92d-6aaf-48cb-9388-9c551daeabd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized usando la norma L^1\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using $L^1$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "l1NormData = normalizer.transform(data)\n",
    "print(\"Normalized usando la norma L^1\")\n",
    "l1NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3b894b-3d28-4f88-b0fb-762673e46908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando usando la norma L^2\n+---+--------------+--------------------+\n| id|      features|              norma2|\n+---+--------------+--------------------+\n|  0|[1.0,0.5,-1.0]|[0.66666666666666...|\n|  1| [2.0,1.0,1.0]|[0.81649658092772...|\n|  2|[4.0,10.0,2.0]|[0.36514837167011...|\n+---+--------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using $L^2$ norm.\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"norma2\", p=2.0)\n",
    "l2NormData = normalizer.transform(data)\n",
    "print(\"Normalizando usando la norma L^2\")\n",
    "l2NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a45b2faf-5c19-4988-89fb-66752073b486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando usando la norma L^inf\n+---+--------------+--------------+\n| id|      features|        norma2|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Normalize each Vector using $L^\\infty$ norm.\n",
    "lInfNormData = normalizer.transform(data, {normalizer.p: float(\"inf\")})\n",
    "print(\"Normalizando usando la norma L^inf\")\n",
    "lInfNormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c9187b-1e09-4e23-bb58-7eee1cb1ee2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "# Creando un object de la class Normalizer\n",
    "ManhattanDistance=Normalizer().setP(1).setInputCol(\"features\").setOutputCol(\"Manhattan Distance\")\n",
    "EuclideanDistance=Normalizer().setP(2).setInputCol(\"features\").setOutputCol(\"Euclidean Distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54562bb2-d8eb-452b-85bd-b6d26984d668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----+----+\n|address|          date|name|food|\n+-------+--------------+----+----+\n|1111111|20151122045510| Yin| gre|\n|1111111|20151122045501| Yin| gre|\n|1111111|20151122045500| Yln| gra|\n|1111112|20151122065832| Yun| ddd|\n|1111113|20160101003221| Yan| fdf|\n|1111111|20160703045231| Yin| gre|\n|1111114|20150419134543| Yin| fdf|\n|1111115|20151123174302| Yen| ddd|\n|2111115|      20123192| Yen| gre|\n+-------+--------------+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1111111,20151122045510, \"Yin\",\"gre\"), (1111111,20151122045501, \"Yin\",\"gre\"), (1111111,20151122045500, \"Yln\",\"gra\")\n",
    "     , (1111112,20151122065832, \"Yun\",\"ddd\"), (1111113,20160101003221, \"Yan\",\"fdf\"), (1111111,20160703045231, \"Yin\",\"gre\"),\n",
    "    (1111114,20150419134543, \"Yin\",\"fdf\"), (1111115,20151123174302, \"Yen\",\"ddd\"),(2111115, 20123192, \"Yen\",\"gre\")],\n",
    "    [\"address\", \"date\",\"name\",\"food\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd9b14d-969a-4e40-815a-7335dfe339d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----+----+----------+\n|address|          date|name|food|name_index|\n+-------+--------------+----+----+----------+\n|1111111|20151122045510| Yin| gre|       0.0|\n|1111111|20151122045501| Yin| gre|       0.0|\n|1111111|20151122045500| Yln| gra|       3.0|\n|1111112|20151122065832| Yun| ddd|       4.0|\n|1111113|20160101003221| Yan| fdf|       2.0|\n|1111111|20160703045231| Yin| gre|       0.0|\n|1111114|20150419134543| Yin| fdf|       0.0|\n|1111115|20151123174302| Yen| ddd|       1.0|\n|2111115|      20123192| Yen| gre|       1.0|\n+-------+--------------+----+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"name\", outputCol=\"name_index\").fit(df)\n",
    "df_ind = indexer.transform(df)\n",
    "df_ind.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c9197c9-8ded-4463-8526-d73868cbaaca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----+----+----------+----------+-------------+\n|address|          date|name|food|food_index|name_index|address_index|\n+-------+--------------+----+----+----------+----------+-------------+\n|1111111|20151122045510| Yin| gre|       0.0|       0.0|          0.0|\n|1111111|20151122045501| Yin| gre|       0.0|       0.0|          0.0|\n|1111111|20151122045500| Yln| gra|       3.0|       3.0|          0.0|\n|1111112|20151122065832| Yun| ddd|       1.0|       4.0|          1.0|\n|1111113|20160101003221| Yan| fdf|       2.0|       2.0|          2.0|\n|1111111|20160703045231| Yin| gre|       0.0|       0.0|          0.0|\n|1111114|20150419134543| Yin| fdf|       2.0|       0.0|          3.0|\n|1111115|20151123174302| Yen| ddd|       1.0|       1.0|          4.0|\n|2111115|      20123192| Yen| gre|       0.0|       1.0|          5.0|\n+-------+--------------+----+----+----------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [ StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df)\n",
    "            for column in list(set(df.columns)-set(['date'])) ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf73807-7d29-46af-9fc1-d0fc1a962f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------------+-------+\n| id|hour|mobile|  userFeatures|clicked|\n+---+----+------+--------------+-------+\n|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n|  1|  20|   1.0|[0.0,15.0,0.2]|    5.0|\n|  1|  15|   3.0|[0.0,11.0,0.8]|    7.0|\n|  0|  10|   5.0|[0.3,11.0,0.6]|    4.0|\n|  5|   3|   5.0|[0.7,19.0,0.1]|    3.3|\n+---+----+------+--------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "dataset = spark.createDataFrame(\n",
    "    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0),(1, 20, 1.0, Vectors.dense([0.0, 15.0, 0.2]), 5.0),\n",
    "    (1, 15, 3.0, Vectors.dense([0.0, 11.0, 0.8]), 7.0), (0, 10, 5.0, Vectors.dense([0.3, 11.0, 0.6]), 4.0),\n",
    "    (5, 3, 5.0, Vectors.dense([0.7, 19.0, 0.1]), 3.3)],\n",
    "    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b68d99a8-011e-47d7-82ed-5e536e213fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n+-----------------------+-------+\n|vector                 |clicked|\n+-----------------------+-------+\n|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n|[20.0,1.0,0.0,15.0,0.2]|5.0    |\n|[15.0,3.0,0.0,11.0,0.8]|7.0    |\n|[10.0,5.0,0.3,11.0,0.6]|4.0    |\n|[3.0,5.0,0.7,19.0,0.1] |3.3    |\n+-----------------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"vector\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"vector\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c595676-158f-4eef-882d-cc59465c9c48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------------+-------+--------------------+\n| id|hour|mobile|  userFeatures|clicked|            features|\n+---+----+------+--------------+-------+--------------------+\n|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|[18.0,1.0,0.0,10....|\n|  1|  20|   1.0|[0.0,15.0,0.2]|    5.0|[20.0,1.0,0.0,15....|\n|  1|  15|   3.0|[0.0,11.0,0.8]|    7.0|[15.0,3.0,0.0,11....|\n|  0|  10|   5.0|[0.3,11.0,0.6]|    4.0|[10.0,5.0,0.3,11....|\n|  5|   3|   5.0|[0.7,19.0,0.1]|    3.3|[3.0,5.0,0.7,19.0...|\n+---+----+------+--------------+-------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Creamos el vector\n",
    "assembler2 = VectorAssembler(inputCols=[\"hour\", \"mobile\", \"userFeatures\"], outputCol=\"features\").setParams(handleInvalid=\"skip\")# omita los valore nulos\n",
    "assembler2.transform(dataset).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a9cd303-551f-4499-8429-f6964707d1f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|features|\n+---+--------+\n|  A|  -99.99|\n|  B|    -0.5|\n|  C|    -0.3|\n|  D|     0.0|\n|  E|     0.7|\n|  F|   99.99|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('A', -99.99), ('B', -0.5), ('C', -0.3),('D', 0.0), ('E', 0.7), ('F', 99.99)]\n",
    "\n",
    "dataframe = spark.createDataFrame(data, [\"id\", \"features\"])\n",
    "dataframe.show()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f02ca1-cca9-41c0-9001-7deb403f42fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucketizer output with 4 buckets\n+---+--------+----------------+\n| id|features|bucketedFeatures|\n+---+--------+----------------+\n|  A|  -99.99|             0.0|\n|  B|    -0.5|             1.0|\n|  C|    -0.3|             1.0|\n|  D|     0.0|             2.0|\n|  E|     0.7|             3.0|\n|  F|   99.99|             3.0|\n+---+--------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n",
    "# Transform original data into its bucket index.\n",
    "bucketedData = bucketizer.transform(dataframe)\n",
    "print(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits()) - 1))\n",
    "bucketedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694b66ac-9397-42c9-a740-3883f57aeaff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.ml.feature import QuantileDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8b4f97c-1d2b-4878-938f-93f3aefe34bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n| id|hour|\n+---+----+\n|  0|18.0|\n|  1|19.0|\n|  2| 8.0|\n|  3| 5.0|\n|  4| 2.2|\n+---+----+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"hour\"])\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d076f434-4f6e-4862-aebc-b1872eb34b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+\n| id|hour|buckets|\n+---+----+-------+\n|  0|18.0|    4.0|\n|  1|19.0|    4.0|\n|  2| 8.0|    3.0|\n|  3| 5.0|    2.0|\n|  4| 2.2|    1.0|\n+---+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "qds = QuantileDiscretizer(numBuckets=5, inputCol=\"hour\",outputCol=\"buckets\", relativeError=0.01, handleInvalid=\"error\")\n",
    "bucketizer = qds.fit(df)\n",
    "bucketizer.setHandleInvalid(\"skip\").transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8fefb82-f001-47b3-98c0-1071d319086c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n| gene|value|\n+-----+-----+\n|gene1|  1.2|\n|gene2|  3.4|\n|gene1|  3.5|\n|gene2| 12.6|\n+-----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data = [('gene1', 1.2), ('gene2', 3.4), ('gene1', 3.5), ('gene2', 12.6)]\n",
    "df = spark.createDataFrame(data, [\"gene\", \"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec80c22-ee12-44db-b345-84c7ac005a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------+------------------+\n| gene|value|           base-10|            base-e|\n+-----+-----+------------------+------------------+\n|gene1|  1.2|0.0791812460476248|0.1823215567939546|\n|gene2|  3.4| 0.531478917042255|1.2237754316221157|\n|gene1|  3.5|0.5440680443502756| 1.252762968495368|\n|gene2| 12.6|1.1003705451175627| 2.533696813957432|\n+-----+-----+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import log\n",
    "df.withColumn(\"base-10\", log(10.0, df.value)).withColumn(\"base-e\", log(df.value)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c20b63-547a-43c3-ba38-cc69e998517b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+\n|id |safety_level|engine_type|\n+---+------------+-----------+\n|1  |Very-Low    |v4         |\n|2  |Very-Low    |v6         |\n|3  |Low         |v6         |\n|4  |Low         |v6         |\n|5  |Medium      |v4         |\n|6  |High        |v6         |\n|7  |High        |v6         |\n|8  |Very-High   |v4         |\n|9  |Very-High   |v6         |\n+---+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType().add(\"id\",\"integer\").add(\"safety_level\",\"string\").add(\"engine_type\",\"string\")\n",
    "schema\n",
    "#StructType(list(StructField(id,IntegerType,True),StructField(safety_level,StringType,True),StructField(engine_type,StringType,True)))\n",
    "\n",
    "data = [(1,'Very-Low','v4'),(2,'Very-Low','v6'),(3,'Low','v6'),(4,'Low','v6'),(5,'Medium','v4'),\n",
    "        (6,'High','v6'),(7,'High','v6'),(8,'Very-High','v4'),(9,'Very-High','v6')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79ed6aa-2828-441c-87d8-824f64842597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- safety_level: string (nullable = true)\n |-- engine_type: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc0ba111-0a60-4c1b-9187-d4545fc0e392",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------------+\n| id|safety_level|engine_type|safety_level_index|\n+---+------------+-----------+------------------+\n|  1|    Very-Low|         v4|               3.0|\n|  2|    Very-Low|         v6|               3.0|\n|  3|         Low|         v6|               1.0|\n|  4|         Low|         v6|               1.0|\n|  5|      Medium|         v4|               4.0|\n|  6|        High|         v6|               0.0|\n|  7|        High|         v6|               0.0|\n|  8|   Very-High|         v4|               2.0|\n|  9|   Very-High|         v6|               2.0|\n+---+------------+-----------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "safety_level_indexer = StringIndexer(inputCol=\"safety_level\",outputCol=\"safety_level_index\")\n",
    "df1 = safety_level_indexer.fit(df).transform(df)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11cbea0f-c8f1-46ca-8d79-33b8dbce6e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-----------------+\n| id|safety_level|engine_type|engine_type_index|\n+---+------------+-----------+-----------------+\n|  1|    Very-Low|         v4|              1.0|\n|  2|    Very-Low|         v6|              0.0|\n|  3|         Low|         v6|              0.0|\n|  4|         Low|         v6|              0.0|\n|  5|      Medium|         v4|              1.0|\n|  6|        High|         v6|              0.0|\n|  7|        High|         v6|              0.0|\n|  8|   Very-High|         v4|              1.0|\n|  9|   Very-High|         v6|              0.0|\n+---+------------+-----------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "engine_type_indexer = StringIndexer(inputCol=\"engine_type\",outputCol=\"engine_type_index\")\n",
    "df2 = engine_type_indexer.fit(df).transform(df)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71379489-4645-429c-af67-13ad4fa9c8ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------------+-------------------+\n|id |safety_level|engine_type|safety_level_index|safety_level_vector|\n+---+------------+-----------+------------------+-------------------+\n|1  |Very-Low    |v4         |3.0               |(4,[3],[1.0])      |\n|2  |Very-Low    |v6         |3.0               |(4,[3],[1.0])      |\n|3  |Low         |v6         |1.0               |(4,[1],[1.0])      |\n|4  |Low         |v6         |1.0               |(4,[1],[1.0])      |\n|5  |Medium      |v4         |4.0               |(4,[],[])          |\n|6  |High        |v6         |0.0               |(4,[0],[1.0])      |\n|7  |High        |v6         |0.0               |(4,[0],[1.0])      |\n|8  |Very-High   |v4         |2.0               |(4,[2],[1.0])      |\n|9  |Very-High   |v6         |2.0               |(4,[2],[1.0])      |\n+---+------------+-----------+------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "onehotencoder_safety_level = OneHotEncoder(inputCol=\"safety_level_index\",outputCol=\"safety_level_vector\")\n",
    "#                                Ajusta     Transforma \n",
    "df11 = onehotencoder_safety_level.fit(df1).transform(df1)\n",
    "df11.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6028320e-c07e-4fa8-af2f-2d1020d6212e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+-----------------+------------------+\n|id |safety_level|engine_type|engine_type_index|engine_type_vector|\n+---+------------+-----------+-----------------+------------------+\n|1  |Very-Low    |v4         |1.0              |(1,[],[])         |\n|2  |Very-Low    |v6         |0.0              |(1,[0],[1.0])     |\n|3  |Low         |v6         |0.0              |(1,[0],[1.0])     |\n|4  |Low         |v6         |0.0              |(1,[0],[1.0])     |\n|5  |Medium      |v4         |1.0              |(1,[],[])         |\n|6  |High        |v6         |0.0              |(1,[0],[1.0])     |\n|7  |High        |v6         |0.0              |(1,[0],[1.0])     |\n|8  |Very-High   |v4         |1.0              |(1,[],[])         |\n|9  |Very-High   |v6         |0.0              |(1,[0],[1.0])     |\n+---+------------+-----------+-----------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "onehotencoder_engine_type = OneHotEncoder(inputCol=\"engine_type_index\",outputCol=\"engine_type_vector\")\n",
    "df12 = onehotencoder_engine_type.fit(df2).transform(df2)\n",
    "df12.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e08521e-b582-465f-9239-1665a3e2202f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------------+-----------------+\n| id|safety_level|engine_type|safety_level_index|engine_type_index|\n+---+------------+-----------+------------------+-----------------+\n|  1|    Very-Low|         v4|               3.0|              1.0|\n|  2|    Very-Low|         v6|               3.0|              0.0|\n|  3|         Low|         v6|               1.0|              0.0|\n|  4|         Low|         v6|               1.0|              0.0|\n|  5|      Medium|         v4|               4.0|              1.0|\n|  6|        High|         v6|               0.0|              0.0|\n|  7|        High|         v6|               0.0|              0.0|\n|  8|   Very-High|         v4|               2.0|              1.0|\n|  9|   Very-High|         v6|               2.0|              0.0|\n+---+------------+-----------+------------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in list(set(df.columns)-set(['id'])) ]\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_indexed = pipeline.fit(df).transform(df)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f7f519d-30fc-4486-93bd-db76a65a7024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------------+-----------------+--------------------------+-------------------------+-------------------+\n| id|safety_level|engine_type|safety_level_index|engine_type_index|safety_level_index_encoded|engine_type_index_encoded|           features|\n+---+------------+-----------+------------------+-----------------+--------------------------+-------------------------+-------------------+\n|  1|    Very-Low|         v4|               3.0|              1.0|             (4,[3],[1.0])|                (1,[],[])|      (5,[3],[1.0])|\n|  2|    Very-Low|         v6|               3.0|              0.0|             (4,[3],[1.0])|            (1,[0],[1.0])|(5,[3,4],[1.0,1.0])|\n|  3|         Low|         v6|               1.0|              0.0|             (4,[1],[1.0])|            (1,[0],[1.0])|(5,[1,4],[1.0,1.0])|\n|  4|         Low|         v6|               1.0|              0.0|             (4,[1],[1.0])|            (1,[0],[1.0])|(5,[1,4],[1.0,1.0])|\n|  5|      Medium|         v4|               4.0|              1.0|                 (4,[],[])|                (1,[],[])|          (5,[],[])|\n|  6|        High|         v6|               0.0|              0.0|             (4,[0],[1.0])|            (1,[0],[1.0])|(5,[0,4],[1.0,1.0])|\n|  7|        High|         v6|               0.0|              0.0|             (4,[0],[1.0])|            (1,[0],[1.0])|(5,[0,4],[1.0,1.0])|\n|  8|   Very-High|         v4|               2.0|              1.0|             (4,[2],[1.0])|                (1,[],[])|      (5,[2],[1.0])|\n|  9|   Very-High|         v6|               2.0|              0.0|             (4,[2],[1.0])|            (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|\n+---+------------+-----------+------------------+-----------------+--------------------------+-------------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers],\n",
    "                        outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers])\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=encoder.getOutputCols(),outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler])\n",
    "\n",
    "pipeline.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efdb25f-65e8-440f-b5c7-2b291bca2fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "safety_level_indexer = StringIndexer(inputCol=\"safety_level\",outputCol=\"safety_level_index\")\n",
    "engine_type_indexer = StringIndexer(inputCol=\"engine_type\", outputCol=\"engine_type_index\")\n",
    "onehotencoder_safety_level = OneHotEncoder(inputCol=\"safety_level_index\",outputCol=\"safety_level_vector\")\n",
    "onehotencoder_engine_type = OneHotEncoder(inputCol=\"engine_type_index\",outputCol=\"engine_type_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba28d785-443a-4035-8837-c041917bb5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------------+-----------------+-------------------+------------------+\n|id |safety_level|engine_type|safety_level_index|engine_type_index|safety_level_vector|engine_type_vector|\n+---+------------+-----------+------------------+-----------------+-------------------+------------------+\n|1  |Very-Low    |v4         |3.0               |1.0              |(4,[3],[1.0])      |(1,[],[])         |\n|2  |Very-Low    |v6         |3.0               |0.0              |(4,[3],[1.0])      |(1,[0],[1.0])     |\n|3  |Low         |v6         |1.0               |0.0              |(4,[1],[1.0])      |(1,[0],[1.0])     |\n|4  |Low         |v6         |1.0               |0.0              |(4,[1],[1.0])      |(1,[0],[1.0])     |\n|5  |Medium      |v4         |4.0               |1.0              |(4,[],[])          |(1,[],[])         |\n|6  |High        |v6         |0.0               |0.0              |(4,[0],[1.0])      |(1,[0],[1.0])     |\n|7  |High        |v6         |0.0               |0.0              |(4,[0],[1.0])      |(1,[0],[1.0])     |\n|8  |Very-High   |v4         |2.0               |1.0              |(4,[2],[1.0])      |(1,[],[])         |\n|9  |Very-High   |v6         |2.0               |0.0              |(4,[2],[1.0])      |(1,[0],[1.0])     |\n+---+------------+-----------+------------------+-----------------+-------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=[safety_level_indexer,\n",
    "                            engine_type_indexer, onehotencoder_safety_level, onehotencoder_engine_type])\n",
    "\n",
    "df_transformed = pipeline.fit(df).transform(df)\n",
    "df_transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c79016f-828b-47f0-993e-9f3464d503b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n|  id|            document|\n+----+--------------------+\n|doc1|Ada Ada Spark Spa...|\n|doc2|             Ada SQL|\n+----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "documents = spark.createDataFrame([(\"doc1\", \"Ada Ada Spark Spark Spark\"),(\"doc2\", \"Ada SQL\")],[\"id\", \"document\"])\n",
    "documents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e432ec0f-d768-4e6d-a84c-5e0708aeccaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3848327506660287>:1\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    TF(Ada, doc1) = 2\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m cannot assign to function call\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-3848327506660287>:1\u001B[0;36m\u001B[0m\n\u001B[0;31m    TF(Ada, doc1) = 2\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m cannot assign to function call\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: cannot assign to function call (<command-3848327506660287>, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "TF(Ada, doc1) = 2\n",
    "TF(Spark, doc1) = 3\n",
    "TF(Ada, doc2) = 1\n",
    "TF(SQL, doc2) = 1\n",
    "DF(Ada, D) = 2\n",
    "DF(Spark, D) = 1\n",
    "DF(SQL, D) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5117dfbf-8054-4393-86e9-18faffc2b0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3848327506660288>:1\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    IDF(Ada, D) = log ( (|D|+1) / (DF(t,D)+1) )\u001B[0m\n",
       "\u001B[0m    ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m cannot assign to function call\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-3848327506660288>:1\u001B[0;36m\u001B[0m\n\u001B[0;31m    IDF(Ada, D) = log ( (|D|+1) / (DF(t,D)+1) )\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m cannot assign to function call\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: cannot assign to function call (<command-3848327506660288>, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "IDF(Ada, D) = log ( (|D|+1) / (DF(t,D)+1) )\n",
    "= log ( (2+1) / (DF(Ada, D)+1) )\n",
    "= log ( 3 / (2+1)) = log(1)\n",
    "= 0.00\n",
    "\n",
    "IDF(Spark, D) = log ( (|D|+1) / (DF(t,D)+1) )\n",
    "= log ( (2+1) / (DF(Spark, D)+1) )\n",
    "= log ( 3 / (1+1) )\n",
    "= log (1.5)\n",
    "= 0.40546510811\n",
    "\n",
    "TF-IDF(Ada, doc1, D) = TF(Ada, doc1) x IDF(Ada, D)\n",
    "= 2 x 0.0\n",
    "= 0.0\n",
    "\n",
    "TF-IDF(Spark, doc1, D) = TF(Spark, doc1) x IDF(Spark, D)\n",
    "= 3 x 0.40546510811\n",
    "= 1.21639532433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902a997b-55de-41dc-b938-57243c244405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------+\n|label|text                         |\n+-----+-----------------------------+\n|0.0  |we heard about Spark and Java|\n|0.0  |Does Java use case classes   |\n|1.0  |fox jumped over fence        |\n|1.0  |red fox jumped over          |\n+-----+-----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentences = spark.createDataFrame([ (0.0, \"we heard about Spark and Java\"),(0.0, \"Does Java use case classes\"),\n",
    "                                   (1.0, \"fox jumped over fence\"),(1.0, \"red fox jumped over\")], [\"label\", \"text\"])\n",
    "\n",
    "sentences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879487f6-290a-41a6-b533-aec919fc6678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------+------------------------------------+\n|label|text                         |words                               |\n+-----+-----------------------------+------------------------------------+\n|0.0  |we heard about Spark and Java|[we, heard, about, spark, and, java]|\n|0.0  |Does Java use case classes   |[does, java, use, case, classes]    |\n|1.0  |fox jumped over fence        |[fox, jumped, over, fence]          |\n|1.0  |red fox jumped over          |[red, fox, jumped, over]            |\n+-----+-----------------------------+------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(sentences)\n",
    "words_data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba7c867-a69b-42a0-838d-3c0279222f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------+\n|label|raw_features                                   |\n+-----+-----------------------------------------------+\n|0.0  |(16,[1,4,6,11,12,15],[1.0,1.0,1.0,1.0,1.0,1.0])|\n|0.0  |(16,[2,6,11,13,15],[1.0,1.0,1.0,1.0,1.0])      |\n|1.0  |(16,[0,1,6,8],[1.0,1.0,1.0,1.0])               |\n|1.0  |(16,[1,4,6,8],[1.0,1.0,1.0,1.0])               |\n+-----+-----------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\",numFeatures=16)\n",
    "featurized_data = hashingTF.transform(words_data)\n",
    "featurized_data.select(\"label\", \"raw_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ba5347-279b-4df1-ad3e-703572ac866c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------------------------------------------------------------------------------------------------------+\n|label|features                                                                                                                   |\n+-----+---------------------------------------------------------------------------------------------------------------------------+\n|0.0  |(16,[1,4,6,11,12,15],[0.22314355131420976,0.5108256237659907,0.0,0.5108256237659907,0.9162907318741551,0.5108256237659907])|\n|0.0  |(16,[2,6,11,13,15],[0.9162907318741551,0.0,0.5108256237659907,0.9162907318741551,0.5108256237659907])                      |\n|1.0  |(16,[0,1,6,8],[0.9162907318741551,0.22314355131420976,0.0,0.5108256237659907])                                             |\n|1.0  |(16,[1,4,6,8],[0.22314355131420976,0.5108256237659907,0.0,0.5108256237659907])                                             |\n+-----+---------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "rescaled_data = idf_model.transform(featurized_data)\n",
    "rescaled_data.select(\"label\", \"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45ddb38-c56d-4880-9fc7-54eb0805141c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n|label|            raw|\n+-----+---------------+\n|    0|      [a, b, c]|\n|    1|[a, b, b, c, a]|\n+-----+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])], [\"label\", \"raw\"] )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d867cc8-1c7b-4bfa-9135-f07b716900a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------------+\n|label|raw            |features                 |\n+-----+---------------+-------------------------+\n|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+-----+---------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer().setInputCol(\"raw\").setOutputCol(\"features\")\n",
    "model = cv.fit(df)\n",
    "transformed = model.transform(df)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246c4371-121b-4d2f-a4eb-61aa66684e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+-------------------------------+\n|label|raw            |features                       |\n+-----+---------------+-------------------------------+\n|0    |[a, b, c]      |(128,[40,99,117],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(128,[40,99,117],[1.0,2.0,2.0])|\n+-----+---------------+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "hashing_TF = HashingTF(inputCol=\"raw\", outputCol=\"features\", numFeatures=128)\n",
    "result = hashing_TF.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dd1dd58-e6e9-44b4-b1a4-ed8a08ffdf13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|                text|\n+--------------------+\n|[Hi, I, heard, ab...|\n|[I, wish, Java, c...|\n|[Logistic, regres...|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "\n",
    "documentDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d981318-20c1-47d8-a380-2b21b23c4c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \nVector: [0.012264367192983627,-0.06442034244537354,-0.007622340321540833]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.05160687722465289,0.025969027541577816,0.02736483487699713]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [-0.06564115285873413,0.02060299552977085,-0.08455150425434113]\n\n"
     ]
    }
   ],
   "source": [
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb8ef2c-d6a8-407b-8af3-842cab7d909e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "\n",
    "model = cv.fit(df)\n",
    "\n",
    "result = model.transform(df)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a6b1f5-c3b5-4582-8cc1-19e49d57aa61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+------+\n|number|boolean|string_number|string|\n+------+-------+-------------+------+\n|   2.1|   true|            1|   fox|\n|   2.1|  false|            2|  gray|\n|   3.3|  false|            2|   red|\n|   4.4|   true|            4|   fox|\n+------+-------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "df = spark.createDataFrame([(2.1, True, \"1\", \"fox\"), (2.1, False, \"2\", \"gray\"), (3.3, False, \"2\", \"red\"),\n",
    "                            (4.4, True, \"4\", \"fox\")], [\"number\", \"boolean\", \"string_number\", \"string\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "080aff52-522d-4268-aa2d-00ed1922ca35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------+------+--------------------------------------------------------+\n|number|boolean|string_number|string|features                                                |\n+------+-------+-------------+------+--------------------------------------------------------+\n|2.1   |true   |1            |fox   |(262144,[102440,112150,135239,185244],[1.0,1.0,2.1,1.0])|\n|2.1   |false  |2            |gray  |(262144,[43117,93531,135239,210818],[1.0,1.0,2.1,1.0])  |\n|3.3   |false  |2            |red   |(262144,[93531,110541,135239,210818],[1.0,1.0,3.3,1.0]) |\n|4.4   |true   |4            |fox   |(262144,[75860,102440,135239,185244],[1.0,1.0,4.4,1.0]) |\n+------+-------+-------------+------+--------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "input_columns = [\"number\", \"boolean\", \"string_number\", \"string\"]\n",
    "hasher = FeatureHasher(inputCols=input_columns, outputCol=\"features\")\n",
    "#hasher.setInputCols(input_columns)\n",
    "featurized = hasher.transform(df)\n",
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6179eaa8-65bc-45f4-8013-50c0daf0d813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n| id|dept|salary|\n+---+----+------+\n| 10|  d1| 27000|\n| 20|  d1| 29000|\n| 40|  d2| 31000|\n| 50|  d2| 39000|\n+---+----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "df = spark.createDataFrame([(10, \"d1\", 27000),(20, \"d1\", 29000),\n",
    "                                (40, \"d2\", 31000),(50, \"d2\", 39000)], \n",
    "                               [\"id\", \"dept\", \"salary\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c381223b-ea8a-4fc4-8d7d-d4df8524a956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n|dept|sum_of_salary|\n+----+-------------+\n|  d1|        56000|\n|  d2|        70000|\n+----+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "query = \"SELECT dept, SUM(salary) AS sum_of_salary FROM __THIS__ GROUP BY dept\"\n",
    "sqlTrans = SQLTransformer(statement=query)\n",
    "sqlTrans.transform(df).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spak05 MIK 2025-07-18 14:09:02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}